{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lopezkor000/applied-llms/blob/main/ApliedLLMModule_1Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjPMv_LwDiod"
      },
      "outputs": [],
      "source": [
        "# Home of AI community Hugging Face, it's going to be our home too (open source)\n",
        "# contributors include Meta Google Deepseek and many more\n",
        "\n",
        "# https://huggingface.co/\n",
        "\n",
        "\n",
        "# Models, Datasets, Applications(Spaces)\n",
        "\n",
        "\n",
        "# For Python use it provides core ML Libraries\n",
        "\n",
        "# The one most likely we will work with\n",
        "\n",
        "# transformers\n",
        "# tokenizers\n",
        "# sentence transformers aka SBERT\n",
        "# datasets\n",
        "# evaluate\n",
        "# PEFT\n",
        "# Bitsandbytes\n",
        "# TRL\n",
        "# This is a base or core I want to introduce to you\n",
        "\n",
        "# https://huggingface.co/docs | You can't beat the documentation\n",
        "# https://huggingface.co/learn | Bookmark this\n",
        "# https://huggingface.co/learn/llm-course/ | core reading will be this\n",
        "# also expect couple of papers and youtube videos :)\n",
        "\n",
        "# each class we will start with 15-20 min quiz\n",
        "\n",
        "# a) google colab will work for quick exploration\n",
        "# https://colab.google/\n",
        "\n",
        "# b) for heavy tasks where gpu acceleration is needed, you will need\n",
        "# kaggle account with it's 30 hours gpu compute per week for free should be enough\n",
        "# https://www.kaggle.com/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Explore\n",
        "\n",
        "# go to\n",
        "# https://huggingface.co/models\n",
        "\n",
        "# On left side you will TASK\n",
        "# where you can see many AI task\n",
        "\n",
        "# let's start with [Text Generation]\n",
        "# Set Parameters up to 6b\n",
        "# sort them by trending you should see\n",
        "# many familiar names like Meta, Google, Salesforce,Qwen and so on\n",
        "\n",
        "# let's choose\n",
        "# https://huggingface.co/Qwen/Qwen3-0.6B\n",
        "\n",
        "\n",
        "\n",
        "# this one you will do at home, since it will take more time\n",
        "# https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507\n",
        "#\n",
        "\n",
        "# I know model page looks intimidating, but only for a couple of months\n",
        "# then you just will look for information you need only\n",
        "\n",
        "# so quickest way to test the model is to click on [Use this model]\n",
        "\n",
        "# Let's start with Local Inference Ideas\n",
        "\n",
        "# let so do google colab\n",
        "\n"
      ],
      "metadata": {
        "id": "spmNoRW9FJCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -U transformers\n",
        "\n",
        "# our best friend transformers library we can use"
      ],
      "metadata": {
        "id": "bAFbLOMpO5kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so this like most easiest way, so first time you use\n",
        "# it may ask for hugging face token\n",
        "# and download the weights in cache\n",
        "\n",
        "# Most friendly and simple way to use.\n",
        "# downside everything is harcoded, no control\n",
        "\n",
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "7T0P-ENUO-xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# so we will encounter many libs and strange naming so beside llm to ask\n",
        "# you may sometimes inspect code more directly\n",
        "\n",
        "import inspect\n",
        "\n",
        "print(inspect.getsource(pipeline))\n",
        "\n",
        "# we will observe most of [task] today,\n",
        "\n",
        "# take a look at text-generation which returns TextGenerationPipeline`\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9vklYeJ5Putb",
        "outputId": "b2b2d3b7-800d-41b8-cc2f-b033bfa653d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def pipeline(\n",
            "    task: Optional[str] = None,\n",
            "    model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None,\n",
            "    config: Optional[Union[str, PretrainedConfig]] = None,\n",
            "    tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None,\n",
            "    feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None,\n",
            "    image_processor: Optional[Union[str, BaseImageProcessor]] = None,\n",
            "    processor: Optional[Union[str, ProcessorMixin]] = None,\n",
            "    framework: Optional[str] = None,\n",
            "    revision: Optional[str] = None,\n",
            "    use_fast: bool = True,\n",
            "    token: Optional[Union[str, bool]] = None,\n",
            "    device: Optional[Union[int, str, \"torch.device\"]] = None,\n",
            "    device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None,\n",
            "    dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\",\n",
            "    trust_remote_code: Optional[bool] = None,\n",
            "    model_kwargs: Optional[dict[str, Any]] = None,\n",
            "    pipeline_class: Optional[Any] = None,\n",
            "    **kwargs: Any,\n",
            ") -> Pipeline:\n",
            "    \"\"\"\n",
            "    Utility factory method to build a [`Pipeline`].\n",
            "\n",
            "    A pipeline consists of:\n",
            "\n",
            "        - One or more components for pre-processing model inputs, such as a [tokenizer](tokenizer),\n",
            "        [image_processor](image_processor), [feature_extractor](feature_extractor), or [processor](processors).\n",
            "        - A [model](model) that generates predictions from the inputs.\n",
            "        - Optional post-processing steps to refine the model's output, which can also be handled by processors.\n",
            "\n",
            "    <Tip>\n",
            "    While there are such optional arguments as `tokenizer`, `feature_extractor`, `image_processor`, and `processor`,\n",
            "    they shouldn't be specified all at once. If these components are not provided, `pipeline` will try to load\n",
            "    required ones automatically. In case you want to provide these components explicitly, please refer to a\n",
            "    specific pipeline in order to get more details regarding what components are required.\n",
            "    </Tip>\n",
            "\n",
            "    Args:\n",
            "        task (`str`):\n",
            "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
            "\n",
            "            - `\"audio-classification\"`: will return a [`AudioClassificationPipeline`].\n",
            "            - `\"automatic-speech-recognition\"`: will return a [`AutomaticSpeechRecognitionPipeline`].\n",
            "            - `\"depth-estimation\"`: will return a [`DepthEstimationPipeline`].\n",
            "            - `\"document-question-answering\"`: will return a [`DocumentQuestionAnsweringPipeline`].\n",
            "            - `\"feature-extraction\"`: will return a [`FeatureExtractionPipeline`].\n",
            "            - `\"fill-mask\"`: will return a [`FillMaskPipeline`]:.\n",
            "            - `\"image-classification\"`: will return a [`ImageClassificationPipeline`].\n",
            "            - `\"image-feature-extraction\"`: will return an [`ImageFeatureExtractionPipeline`].\n",
            "            - `\"image-segmentation\"`: will return a [`ImageSegmentationPipeline`].\n",
            "            - `\"image-text-to-text\"`: will return a [`ImageTextToTextPipeline`].\n",
            "            - `\"image-to-image\"`: will return a [`ImageToImagePipeline`].\n",
            "            - `\"image-to-text\"`: will return a [`ImageToTextPipeline`].\n",
            "            - `\"keypoint-matching\"`: will return a [`KeypointMatchingPipeline`].\n",
            "            - `\"mask-generation\"`: will return a [`MaskGenerationPipeline`].\n",
            "            - `\"object-detection\"`: will return a [`ObjectDetectionPipeline`].\n",
            "            - `\"question-answering\"`: will return a [`QuestionAnsweringPipeline`].\n",
            "            - `\"summarization\"`: will return a [`SummarizationPipeline`].\n",
            "            - `\"table-question-answering\"`: will return a [`TableQuestionAnsweringPipeline`].\n",
            "            - `\"text2text-generation\"`: will return a [`Text2TextGenerationPipeline`].\n",
            "            - `\"text-classification\"` (alias `\"sentiment-analysis\"` available): will return a\n",
            "              [`TextClassificationPipeline`].\n",
            "            - `\"text-generation\"`: will return a [`TextGenerationPipeline`]:.\n",
            "            - `\"text-to-audio\"` (alias `\"text-to-speech\"` available): will return a [`TextToAudioPipeline`]:.\n",
            "            - `\"token-classification\"` (alias `\"ner\"` available): will return a [`TokenClassificationPipeline`].\n",
            "            - `\"translation\"`: will return a [`TranslationPipeline`].\n",
            "            - `\"translation_xx_to_yy\"`: will return a [`TranslationPipeline`].\n",
            "            - `\"video-classification\"`: will return a [`VideoClassificationPipeline`].\n",
            "            - `\"visual-question-answering\"`: will return a [`VisualQuestionAnsweringPipeline`].\n",
            "            - `\"zero-shot-classification\"`: will return a [`ZeroShotClassificationPipeline`].\n",
            "            - `\"zero-shot-image-classification\"`: will return a [`ZeroShotImageClassificationPipeline`].\n",
            "            - `\"zero-shot-audio-classification\"`: will return a [`ZeroShotAudioClassificationPipeline`].\n",
            "            - `\"zero-shot-object-detection\"`: will return a [`ZeroShotObjectDetectionPipeline`].\n",
            "\n",
            "        model (`str` or [`PreTrainedModel`] or [`TFPreTrainedModel`], *optional*):\n",
            "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
            "            actual instance of a pretrained model inheriting from [`PreTrainedModel`] (for PyTorch) or\n",
            "            [`TFPreTrainedModel`] (for TensorFlow).\n",
            "\n",
            "            If not provided, the default for the `task` will be loaded.\n",
            "        config (`str` or [`PretrainedConfig`], *optional*):\n",
            "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
            "            identifier or an actual pretrained model configuration inheriting from [`PretrainedConfig`].\n",
            "\n",
            "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
            "            `model` is given, its default configuration will be used. However, if `model` is not supplied, this\n",
            "            `task`'s default model's config is used instead.\n",
            "        tokenizer (`str` or [`PreTrainedTokenizer`], *optional*):\n",
            "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
            "            identifier or an actual pretrained tokenizer inheriting from [`PreTrainedTokenizer`].\n",
            "\n",
            "            If not provided, the default tokenizer for the given `model` will be loaded (if it is a string). If `model`\n",
            "            is not specified or not a string, then the default tokenizer for `config` is loaded (if it is a string).\n",
            "            However, if `config` is also not given or not a string, then the default tokenizer for the given `task`\n",
            "            will be loaded.\n",
            "        feature_extractor (`str` or [`PreTrainedFeatureExtractor`], *optional*):\n",
            "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
            "            identifier or an actual pretrained feature extractor inheriting from [`PreTrainedFeatureExtractor`].\n",
            "\n",
            "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
            "            models. Multi-modal models will also require a tokenizer to be passed.\n",
            "\n",
            "            If not provided, the default feature extractor for the given `model` will be loaded (if it is a string). If\n",
            "            `model` is not specified or not a string, then the default feature extractor for `config` is loaded (if it\n",
            "            is a string). However, if `config` is also not given or not a string, then the default feature extractor\n",
            "            for the given `task` will be loaded.\n",
            "        image_processor (`str` or [`BaseImageProcessor`], *optional*):\n",
            "            The image processor that will be used by the pipeline to preprocess images for the model. This can be a\n",
            "            model identifier or an actual image processor inheriting from [`BaseImageProcessor`].\n",
            "\n",
            "            Image processors are used for Vision models and multi-modal models that require image inputs. Multi-modal\n",
            "            models will also require a tokenizer to be passed.\n",
            "\n",
            "            If not provided, the default image processor for the given `model` will be loaded (if it is a string). If\n",
            "            `model` is not specified or not a string, then the default image processor for `config` is loaded (if it is\n",
            "            a string).\n",
            "        processor (`str` or [`ProcessorMixin`], *optional*):\n",
            "            The processor that will be used by the pipeline to preprocess data for the model. This can be a model\n",
            "            identifier or an actual processor inheriting from [`ProcessorMixin`].\n",
            "\n",
            "            Processors are used for multi-modal models that require multi-modal inputs, for example, a model that\n",
            "            requires both text and image inputs.\n",
            "\n",
            "            If not provided, the default processor for the given `model` will be loaded (if it is a string). If `model`\n",
            "            is not specified or not a string, then the default processor for `config` is loaded (if it is a string).\n",
            "        framework (`str`, *optional*):\n",
            "            The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
            "            installed.\n",
            "\n",
            "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
            "            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
            "            provided.\n",
            "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
            "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
            "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
            "            artifacts on huggingface.co, so `revision` can be any identifier allowed by git.\n",
            "        use_fast (`bool`, *optional*, defaults to `True`):\n",
            "            Whether or not to use a Fast tokenizer if possible (a [`PreTrainedTokenizerFast`]).\n",
            "        use_auth_token (`str` or *bool*, *optional*):\n",
            "            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
            "            when running `hf auth login` (stored in `~/.huggingface`).\n",
            "        device (`int` or `str` or `torch.device`):\n",
            "            Defines the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank like `1`) on which this\n",
            "            pipeline will be allocated.\n",
            "        device_map (`str` or `dict[str, Union[int, str, torch.device]`, *optional*):\n",
            "            Sent directly as `model_kwargs` (just a simpler shortcut). When `accelerate` library is present, set\n",
            "            `device_map=\"auto\"` to compute the most optimized `device_map` automatically (see\n",
            "            [here](https://huggingface.co/docs/accelerate/main/en/package_reference/big_modeling#accelerate.cpu_offload)\n",
            "            for more information).\n",
            "\n",
            "            <Tip warning={true}>\n",
            "\n",
            "            Do not use `device_map` AND `device` at the same time as they will conflict\n",
            "\n",
            "            </Tip>\n",
            "\n",
            "        dtype (`str` or `torch.dtype`, *optional*):\n",
            "            Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
            "            (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`).\n",
            "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
            "            Whether or not to allow for custom code defined on the Hub in their own modeling, configuration,\n",
            "            tokenization or even pipeline files. This option should only be set to `True` for repositories you trust\n",
            "            and in which you have read the code, as it will execute code present on the Hub on your local machine.\n",
            "        model_kwargs (`dict[str, Any]`, *optional*):\n",
            "            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,\n",
            "            **model_kwargs)` function.\n",
            "        kwargs (`dict[str, Any]`, *optional*):\n",
            "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
            "            corresponding pipeline class for possible values).\n",
            "\n",
            "    Returns:\n",
            "        [`Pipeline`]: A suitable pipeline for the task.\n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
            "\n",
            "    >>> # Sentiment analysis pipeline\n",
            "    >>> analyzer = pipeline(\"sentiment-analysis\")\n",
            "\n",
            "    >>> # Question answering pipeline, specifying the checkpoint identifier\n",
            "    >>> oracle = pipeline(\n",
            "    ...     \"question-answering\", model=\"distilbert/distilbert-base-cased-distilled-squad\", tokenizer=\"google-bert/bert-base-cased\"\n",
            "    ... )\n",
            "\n",
            "    >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
            "    >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
            "    >>> tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
            "    >>> recognizer = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
            "    ```\"\"\"\n",
            "    if model_kwargs is None:\n",
            "        model_kwargs = {}\n",
            "    # Make sure we only pass use_auth_token once as a kwarg (it used to be possible to pass it in model_kwargs,\n",
            "    # this is to keep BC).\n",
            "    use_auth_token = model_kwargs.pop(\"use_auth_token\", None)\n",
            "    if use_auth_token is not None:\n",
            "        warnings.warn(\n",
            "            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n",
            "            FutureWarning,\n",
            "        )\n",
            "        if token is not None:\n",
            "            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n",
            "        token = use_auth_token\n",
            "\n",
            "    code_revision = kwargs.pop(\"code_revision\", None)\n",
            "    commit_hash = kwargs.pop(\"_commit_hash\", None)\n",
            "\n",
            "    hub_kwargs = {\n",
            "        \"revision\": revision,\n",
            "        \"token\": token,\n",
            "        \"trust_remote_code\": trust_remote_code,\n",
            "        \"_commit_hash\": commit_hash,\n",
            "    }\n",
            "\n",
            "    if task is None and model is None:\n",
            "        raise RuntimeError(\n",
            "            \"Impossible to instantiate a pipeline without either a task or a model \"\n",
            "            \"being specified. \"\n",
            "            \"Please provide a task class or a model\"\n",
            "        )\n",
            "\n",
            "    if model is None and tokenizer is not None:\n",
            "        raise RuntimeError(\n",
            "            \"Impossible to instantiate a pipeline with tokenizer specified but not the model as the provided tokenizer\"\n",
            "            \" may not be compatible with the default model. Please provide a PreTrainedModel class or a\"\n",
            "            \" path/identifier to a pretrained model when providing tokenizer.\"\n",
            "        )\n",
            "    if model is None and feature_extractor is not None:\n",
            "        raise RuntimeError(\n",
            "            \"Impossible to instantiate a pipeline with feature_extractor specified but not the model as the provided\"\n",
            "            \" feature_extractor may not be compatible with the default model. Please provide a PreTrainedModel class\"\n",
            "            \" or a path/identifier to a pretrained model when providing feature_extractor.\"\n",
            "        )\n",
            "    if isinstance(model, Path):\n",
            "        model = str(model)\n",
            "\n",
            "    if commit_hash is None:\n",
            "        pretrained_model_name_or_path = None\n",
            "        if isinstance(config, str):\n",
            "            pretrained_model_name_or_path = config\n",
            "        elif config is None and isinstance(model, str):\n",
            "            pretrained_model_name_or_path = model\n",
            "\n",
            "        if not isinstance(config, PretrainedConfig) and pretrained_model_name_or_path is not None:\n",
            "            # We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\n",
            "            resolved_config_file = cached_file(\n",
            "                pretrained_model_name_or_path,\n",
            "                CONFIG_NAME,\n",
            "                _raise_exceptions_for_gated_repo=False,\n",
            "                _raise_exceptions_for_missing_entries=False,\n",
            "                _raise_exceptions_for_connection_errors=False,\n",
            "                cache_dir=model_kwargs.get(\"cache_dir\"),\n",
            "                **hub_kwargs,\n",
            "            )\n",
            "            hub_kwargs[\"_commit_hash\"] = extract_commit_hash(resolved_config_file, commit_hash)\n",
            "        else:\n",
            "            hub_kwargs[\"_commit_hash\"] = getattr(config, \"_commit_hash\", None)\n",
            "\n",
            "    # Config is the primordial information item.\n",
            "    # Instantiate config if needed\n",
            "    adapter_path = None\n",
            "    if isinstance(config, str):\n",
            "        config = AutoConfig.from_pretrained(\n",
            "            config, _from_pipeline=task, code_revision=code_revision, **hub_kwargs, **model_kwargs\n",
            "        )\n",
            "        hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
            "    elif config is None and isinstance(model, str):\n",
            "        # Check for an adapter file in the model path if PEFT is available\n",
            "        if is_peft_available():\n",
            "            # `find_adapter_config_file` doesn't accept `trust_remote_code`\n",
            "            _hub_kwargs = {k: v for k, v in hub_kwargs.items() if k != \"trust_remote_code\"}\n",
            "            maybe_adapter_path = find_adapter_config_file(\n",
            "                model,\n",
            "                token=hub_kwargs[\"token\"],\n",
            "                revision=hub_kwargs[\"revision\"],\n",
            "                _commit_hash=hub_kwargs[\"_commit_hash\"],\n",
            "            )\n",
            "\n",
            "            if maybe_adapter_path is not None:\n",
            "                with open(maybe_adapter_path, \"r\", encoding=\"utf-8\") as f:\n",
            "                    adapter_config = json.load(f)\n",
            "                    adapter_path = model\n",
            "                    model = adapter_config[\"base_model_name_or_path\"]\n",
            "\n",
            "        config = AutoConfig.from_pretrained(\n",
            "            model, _from_pipeline=task, code_revision=code_revision, **hub_kwargs, **model_kwargs\n",
            "        )\n",
            "        hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
            "\n",
            "    custom_tasks = {}\n",
            "    if config is not None and len(getattr(config, \"custom_pipelines\", {})) > 0:\n",
            "        custom_tasks = config.custom_pipelines\n",
            "        if task is None and trust_remote_code is not False:\n",
            "            if len(custom_tasks) == 1:\n",
            "                task = list(custom_tasks.keys())[0]\n",
            "            else:\n",
            "                raise RuntimeError(\n",
            "                    \"We can't infer the task automatically for this model as there are multiple tasks available. Pick \"\n",
            "                    f\"one in {', '.join(custom_tasks.keys())}\"\n",
            "                )\n",
            "\n",
            "    if task is None and model is not None:\n",
            "        if not isinstance(model, str):\n",
            "            raise RuntimeError(\n",
            "                \"Inferring the task automatically requires to check the hub with a model_id defined as a `str`. \"\n",
            "                f\"{model} is not a valid model_id.\"\n",
            "            )\n",
            "        task = get_task(model, token)\n",
            "\n",
            "    # Retrieve the task\n",
            "    if task in custom_tasks:\n",
            "        targeted_task, task_options = clean_custom_task(custom_tasks[task])\n",
            "        if pipeline_class is None:\n",
            "            if not trust_remote_code:\n",
            "                raise ValueError(\n",
            "                    \"Loading this pipeline requires you to execute the code in the pipeline file in that\"\n",
            "                    \" repo on your local machine. Make sure you have read the code there to avoid malicious use, then\"\n",
            "                    \" set the option `trust_remote_code=True` to remove this error.\"\n",
            "                )\n",
            "            class_ref = targeted_task[\"impl\"]\n",
            "            pipeline_class = get_class_from_dynamic_module(\n",
            "                class_ref,\n",
            "                model,\n",
            "                code_revision=code_revision,\n",
            "                **hub_kwargs,\n",
            "            )\n",
            "    else:\n",
            "        normalized_task, targeted_task, task_options = check_task(task)\n",
            "        if pipeline_class is None:\n",
            "            pipeline_class = targeted_task[\"impl\"]\n",
            "\n",
            "    # Use default model/config/tokenizer for the task if no model is provided\n",
            "    if model is None:\n",
            "        # At that point framework might still be undetermined\n",
            "        model, default_revision = get_default_model_and_revision(targeted_task, framework, task_options)\n",
            "        revision = revision if revision is not None else default_revision\n",
            "        logger.warning(\n",
            "            f\"No model was supplied, defaulted to {model} and revision\"\n",
            "            f\" {revision} ({HUGGINGFACE_CO_RESOLVE_ENDPOINT}/{model}).\\n\"\n",
            "            \"Using a pipeline without specifying a model name and revision in production is not recommended.\"\n",
            "        )\n",
            "        hub_kwargs[\"revision\"] = revision\n",
            "        if config is None and isinstance(model, str):\n",
            "            config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)\n",
            "            hub_kwargs[\"_commit_hash\"] = config._commit_hash\n",
            "\n",
            "    if device_map is not None:\n",
            "        if \"device_map\" in model_kwargs:\n",
            "            raise ValueError(\n",
            "                'You cannot use both `pipeline(... device_map=..., model_kwargs={\"device_map\":...})` as those'\n",
            "                \" arguments might conflict, use only one.)\"\n",
            "            )\n",
            "        if device is not None:\n",
            "            logger.warning(\n",
            "                \"Both `device` and `device_map` are specified. `device` will override `device_map`. You\"\n",
            "                \" will most likely encounter unexpected behavior. Please remove `device` and keep `device_map`.\"\n",
            "            )\n",
            "        model_kwargs[\"device_map\"] = device_map\n",
            "\n",
            "    # BC for the `torch_dtype` argument\n",
            "    if (torch_dtype := kwargs.get(\"torch_dtype\")) is not None:\n",
            "        logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n",
            "        # If both are provided, keep `dtype`\n",
            "        dtype = torch_dtype if dtype == \"auto\" else dtype\n",
            "    if \"torch_dtype\" in model_kwargs or \"dtype\" in model_kwargs:\n",
            "        if \"torch_dtype\" in model_kwargs:\n",
            "            logger.warning_once(\"`torch_dtype` is deprecated! Use `dtype` instead!\")\n",
            "        # If the user did not explicitly provide `dtype` (i.e. the function default \"auto\" is still\n",
            "        # present) but a value is supplied inside `model_kwargs`, we silently defer to the latter instead of\n",
            "        # raising. This prevents false positives like providing `dtype` only via `model_kwargs` while the\n",
            "        # top-level argument keeps its default value \"auto\".\n",
            "        if dtype == \"auto\":\n",
            "            dtype = None\n",
            "        else:\n",
            "            raise ValueError(\n",
            "                'You cannot use both `pipeline(... dtype=..., model_kwargs={\"dtype\":...})` as those'\n",
            "                \" arguments might conflict, use only one.)\"\n",
            "            )\n",
            "    if dtype is not None:\n",
            "        if isinstance(dtype, str) and hasattr(torch, dtype):\n",
            "            dtype = getattr(torch, dtype)\n",
            "        model_kwargs[\"dtype\"] = dtype\n",
            "\n",
            "    model_name = model if isinstance(model, str) else None\n",
            "\n",
            "    # Load the correct model if possible\n",
            "    # Infer the framework from the model if not already defined\n",
            "    if isinstance(model, str) or framework is None:\n",
            "        model_classes = {\"tf\": targeted_task[\"tf\"], \"pt\": targeted_task[\"pt\"]}\n",
            "        framework, model = infer_framework_load_model(\n",
            "            adapter_path if adapter_path is not None else model,\n",
            "            model_classes=model_classes,\n",
            "            config=config,\n",
            "            framework=framework,\n",
            "            task=task,\n",
            "            **hub_kwargs,\n",
            "            **model_kwargs,\n",
            "        )\n",
            "\n",
            "    hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n",
            "\n",
            "    # Check which preprocessing classes the pipeline uses\n",
            "    # None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\n",
            "    load_tokenizer = pipeline_class._load_tokenizer\n",
            "    load_feature_extractor = pipeline_class._load_feature_extractor\n",
            "    load_image_processor = pipeline_class._load_image_processor\n",
            "    load_processor = pipeline_class._load_processor\n",
            "\n",
            "    if load_tokenizer or load_tokenizer is None:\n",
            "        try:\n",
            "            # Try to infer tokenizer from model or config name (if provided as str)\n",
            "            if tokenizer is None:\n",
            "                if isinstance(model_name, str):\n",
            "                    tokenizer = model_name\n",
            "                elif isinstance(config, str):\n",
            "                    tokenizer = config\n",
            "                else:\n",
            "                    # Impossible to guess what is the right tokenizer here\n",
            "                    raise Exception(\n",
            "                        \"Impossible to guess which tokenizer to use. \"\n",
            "                        \"Please provide a PreTrainedTokenizer class or a path/identifier to a pretrained tokenizer.\"\n",
            "                    )\n",
            "\n",
            "            # Instantiate tokenizer if needed\n",
            "            if isinstance(tokenizer, (str, tuple)):\n",
            "                if isinstance(tokenizer, tuple):\n",
            "                    # For tuple we have (tokenizer name, {kwargs})\n",
            "                    use_fast = tokenizer[1].pop(\"use_fast\", use_fast)\n",
            "                    tokenizer_identifier = tokenizer[0]\n",
            "                    tokenizer_kwargs = tokenizer[1]\n",
            "                else:\n",
            "                    tokenizer_identifier = tokenizer\n",
            "                    tokenizer_kwargs = model_kwargs.copy()\n",
            "                    tokenizer_kwargs.pop(\"torch_dtype\", None), tokenizer_kwargs.pop(\"dtype\", None)\n",
            "\n",
            "                tokenizer = AutoTokenizer.from_pretrained(\n",
            "                    tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs\n",
            "                )\n",
            "        except Exception as e:\n",
            "            if load_tokenizer:\n",
            "                raise e\n",
            "            else:\n",
            "                tokenizer = None\n",
            "\n",
            "    if load_image_processor or load_image_processor is None:\n",
            "        try:\n",
            "            # Try to infer image processor from model or config name (if provided as str)\n",
            "            if image_processor is None:\n",
            "                if isinstance(model_name, str):\n",
            "                    image_processor = model_name\n",
            "                elif isinstance(config, str):\n",
            "                    image_processor = config\n",
            "                # Backward compatibility, as `feature_extractor` used to be the name\n",
            "                # for `ImageProcessor`.\n",
            "                elif feature_extractor is not None and isinstance(feature_extractor, BaseImageProcessor):\n",
            "                    image_processor = feature_extractor\n",
            "                else:\n",
            "                    # Impossible to guess what is the right image_processor here\n",
            "                    raise Exception(\n",
            "                        \"Impossible to guess which image processor to use. \"\n",
            "                        \"Please provide a PreTrainedImageProcessor class or a path/identifier \"\n",
            "                        \"to a pretrained image processor.\"\n",
            "                    )\n",
            "\n",
            "            # Instantiate image_processor if needed\n",
            "            if isinstance(image_processor, (str, tuple)):\n",
            "                image_processor = AutoImageProcessor.from_pretrained(\n",
            "                    image_processor, _from_pipeline=task, **hub_kwargs, **model_kwargs\n",
            "                )\n",
            "        except Exception as e:\n",
            "            if load_image_processor:\n",
            "                raise e\n",
            "            else:\n",
            "                image_processor = None\n",
            "\n",
            "    if load_feature_extractor or load_feature_extractor is None:\n",
            "        try:\n",
            "            # Try to infer feature extractor from model or config name (if provided as str)\n",
            "            if feature_extractor is None:\n",
            "                if isinstance(model_name, str):\n",
            "                    feature_extractor = model_name\n",
            "                elif isinstance(config, str):\n",
            "                    feature_extractor = config\n",
            "                else:\n",
            "                    # Impossible to guess what is the right feature_extractor here\n",
            "                    raise Exception(\n",
            "                        \"Impossible to guess which feature extractor to use. \"\n",
            "                        \"Please provide a PreTrainedFeatureExtractor class or a path/identifier \"\n",
            "                        \"to a pretrained feature extractor.\"\n",
            "                    )\n",
            "\n",
            "            # Instantiate feature_extractor if needed\n",
            "            if isinstance(feature_extractor, (str, tuple)):\n",
            "                feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
            "                    feature_extractor, _from_pipeline=task, **hub_kwargs, **model_kwargs\n",
            "                )\n",
            "\n",
            "                if (\n",
            "                    feature_extractor._processor_class\n",
            "                    and feature_extractor._processor_class.endswith(\"WithLM\")\n",
            "                    and isinstance(model_name, str)\n",
            "                ):\n",
            "                    try:\n",
            "                        import kenlm  # to trigger `ImportError` if not installed\n",
            "                        from pyctcdecode import BeamSearchDecoderCTC\n",
            "\n",
            "                        if os.path.isdir(model_name) or os.path.isfile(model_name):\n",
            "                            decoder = BeamSearchDecoderCTC.load_from_dir(model_name)\n",
            "                        else:\n",
            "                            language_model_glob = os.path.join(\n",
            "                                BeamSearchDecoderCTC._LANGUAGE_MODEL_SERIALIZED_DIRECTORY, \"*\"\n",
            "                            )\n",
            "                            alphabet_filename = BeamSearchDecoderCTC._ALPHABET_SERIALIZED_FILENAME\n",
            "                            allow_patterns = [language_model_glob, alphabet_filename]\n",
            "                            decoder = BeamSearchDecoderCTC.load_from_hf_hub(model_name, allow_patterns=allow_patterns)\n",
            "\n",
            "                        kwargs[\"decoder\"] = decoder\n",
            "                    except ImportError as e:\n",
            "                        logger.warning(\n",
            "                            f\"Could not load the `decoder` for {model_name}. Defaulting to raw CTC. Error: {e}\"\n",
            "                        )\n",
            "                        if not is_kenlm_available():\n",
            "                            logger.warning(\"Try to install `kenlm`: `pip install kenlm\")\n",
            "\n",
            "                        if not is_pyctcdecode_available():\n",
            "                            logger.warning(\"Try to install `pyctcdecode`: `pip install pyctcdecode\")\n",
            "        except Exception as e:\n",
            "            if load_feature_extractor:\n",
            "                raise e\n",
            "            else:\n",
            "                feature_extractor = None\n",
            "\n",
            "    if load_processor or load_processor is None:\n",
            "        try:\n",
            "            # Try to infer processor from model or config name (if provided as str)\n",
            "            if processor is None:\n",
            "                if isinstance(model_name, str):\n",
            "                    processor = model_name\n",
            "                elif isinstance(config, str):\n",
            "                    processor = config\n",
            "                else:\n",
            "                    # Impossible to guess what is the right processor here\n",
            "                    raise Exception(\n",
            "                        \"Impossible to guess which processor to use. \"\n",
            "                        \"Please provide a processor instance or a path/identifier \"\n",
            "                        \"to a processor.\"\n",
            "                    )\n",
            "\n",
            "            # Instantiate processor if needed\n",
            "            if isinstance(processor, (str, tuple)):\n",
            "                processor = AutoProcessor.from_pretrained(processor, _from_pipeline=task, **hub_kwargs, **model_kwargs)\n",
            "                if not isinstance(processor, ProcessorMixin):\n",
            "                    raise TypeError(\n",
            "                        \"Processor was loaded, but it is not an instance of `ProcessorMixin`. \"\n",
            "                        f\"Got type `{type(processor)}` instead. Please check that you specified \"\n",
            "                        \"correct pipeline task for the model and model has processor implemented and saved.\"\n",
            "                    )\n",
            "        except Exception as e:\n",
            "            if load_processor:\n",
            "                raise e\n",
            "            else:\n",
            "                processor = None\n",
            "\n",
            "    if task == \"translation\" and model.config.task_specific_params:\n",
            "        for key in model.config.task_specific_params:\n",
            "            if key.startswith(\"translation\"):\n",
            "                task = key\n",
            "                warnings.warn(\n",
            "                    f'\"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"{task}\"',\n",
            "                    UserWarning,\n",
            "                )\n",
            "                break\n",
            "\n",
            "    if tokenizer is not None:\n",
            "        kwargs[\"tokenizer\"] = tokenizer\n",
            "\n",
            "    if feature_extractor is not None:\n",
            "        kwargs[\"feature_extractor\"] = feature_extractor\n",
            "\n",
            "    if dtype is not None:\n",
            "        kwargs[\"dtype\"] = dtype\n",
            "\n",
            "    if image_processor is not None:\n",
            "        kwargs[\"image_processor\"] = image_processor\n",
            "\n",
            "    if device is not None:\n",
            "        kwargs[\"device\"] = device\n",
            "\n",
            "    if processor is not None:\n",
            "        kwargs[\"processor\"] = processor\n",
            "\n",
            "    return pipeline_class(model=model, framework=framework, task=task, **kwargs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import TextGenerationPipeline\n",
        "print(inspect.getsource(TextGenerationPipeline))\n",
        "# you can control many parameters but like still high level api,\n",
        "# for rapid proptype or checking new model just go with it\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_UM3wcQgQdIw",
        "outputId": "e82278df-c596-4208-8b08-18987d46b36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@add_end_docstrings(build_pipeline_init_args(has_tokenizer=True))\n",
            "class TextGenerationPipeline(Pipeline):\n",
            "    \"\"\"\n",
            "    Language generation pipeline using any `ModelWithLMHead` or `ModelForCausalLM`. This pipeline predicts the words\n",
            "    that will follow a specified text prompt. When the underlying model is a conversational model, it can also accept\n",
            "    one or more chats, in which case the pipeline will operate in chat mode and will continue the chat(s) by adding\n",
            "    its response(s). Each chat takes the form of a list of dicts, where each dict contains \"role\" and \"content\" keys.\n",
            "\n",
            "    Unless the model you're using explicitly sets these generation parameters in its configuration files\n",
            "    (`generation_config.json`), the following default values will be used:\n",
            "    - max_new_tokens: 256\n",
            "    - do_sample: True\n",
            "    - temperature: 0.7\n",
            "\n",
            "    Examples:\n",
            "\n",
            "    ```python\n",
            "    >>> from transformers import pipeline\n",
            "\n",
            "    >>> generator = pipeline(model=\"openai-community/gpt2\")\n",
            "    >>> generator(\"I can't believe you did such a \", do_sample=False)\n",
            "    [{'generated_text': \"I can't believe you did such a icky thing to me. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I'm so sorry. I\"}]\n",
            "\n",
            "    >>> # These parameters will return suggestions, and only the newly created text making it easier for prompting suggestions.\n",
            "    >>> outputs = generator(\"My tart needs some\", num_return_sequences=4, return_full_text=False)\n",
            "    ```\n",
            "\n",
            "    ```python\n",
            "    >>> from transformers import pipeline\n",
            "\n",
            "    >>> generator = pipeline(model=\"HuggingFaceH4/zephyr-7b-beta\")\n",
            "    >>> # Zephyr-beta is a conversational model, so let's pass it a chat instead of a single string\n",
            "    >>> generator([{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}], do_sample=False, max_new_tokens=2)\n",
            "    [{'generated_text': [{'role': 'user', 'content': 'What is the capital of France? Answer in one word.'}, {'role': 'assistant', 'content': 'Paris'}]}]\n",
            "    ```\n",
            "\n",
            "    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial). You can pass text\n",
            "    generation parameters to this pipeline to control stopping criteria, decoding strategy, and more. Learn more about\n",
            "    text generation parameters in [Text generation strategies](../generation_strategies) and [Text\n",
            "    generation](text_generation).\n",
            "\n",
            "    This language generation pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
            "    `\"text-generation\"`.\n",
            "\n",
            "    The models that this pipeline can use are models that have been trained with an autoregressive language modeling\n",
            "    objective. See the list of available [text completion models](https://huggingface.co/models?filter=text-generation)\n",
            "    and the list of [conversational models](https://huggingface.co/models?other=conversational)\n",
            "    on [huggingface.co/models].\n",
            "    \"\"\"\n",
            "\n",
            "    # Prefix text to help Transformer-XL and XLNet with short prompts as proposed by Aman Rusia\n",
            "    # in https://github.com/rusiaaman/XLNet-gen#methodology\n",
            "    # and https://medium.com/@amanrusia/xlnet-speaks-comparison-to-gpt-2-ea1a4e9ba39e\n",
            "\n",
            "    XL_PREFIX = \"\"\"\n",
            "    In 1991, the remains of Russian Tsar Nicholas II and his family (except for Alexei and Maria) are discovered. The\n",
            "    voice of Nicholas's young son, Tsarevich Alexei Nikolaevich, narrates the remainder of the story. 1883 Western\n",
            "    Siberia, a young Grigori Rasputin is asked by his father and a group of men to perform magic. Rasputin has a vision\n",
            "    and denounces one of the men as a horse thief. Although his father initially slaps him for making such an\n",
            "    accusation, Rasputin watches as the man is chased outside and beaten. Twenty years later, Rasputin sees a vision of\n",
            "    the Virgin Mary, prompting him to become a priest. Rasputin quickly becomes famous, with people, even a bishop,\n",
            "    begging for his blessing. <eod> </s> <eos>\n",
            "    \"\"\"\n",
            "\n",
            "    _pipeline_calls_generate = True\n",
            "    _load_processor = False\n",
            "    _load_image_processor = False\n",
            "    _load_feature_extractor = False\n",
            "    _load_tokenizer = True\n",
            "\n",
            "    # Make sure the docstring is updated when the default generation config is changed\n",
            "    _default_generation_config = GenerationConfig(\n",
            "        max_new_tokens=256,\n",
            "        do_sample=True,  # free-form text generation often uses sampling\n",
            "        temperature=0.7,\n",
            "    )\n",
            "\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super().__init__(*args, **kwargs)\n",
            "        self.check_model_type(\n",
            "            TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES if self.framework == \"tf\" else MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n",
            "        )\n",
            "        if \"prefix\" not in self._preprocess_params:\n",
            "            # This is very specific. The logic is quite complex and needs to be done\n",
            "            # as a \"default\".\n",
            "            # It also defines both some preprocess_kwargs and generate_kwargs\n",
            "            # which is why we cannot put them in their respective methods.\n",
            "            prefix = None\n",
            "            if self.prefix is not None:\n",
            "                prefix = self.prefix\n",
            "            if prefix is None and self.model.__class__.__name__ in [\n",
            "                \"XLNetLMHeadModel\",\n",
            "                \"TransfoXLLMHeadModel\",\n",
            "                \"TFXLNetLMHeadModel\",\n",
            "                \"TFTransfoXLLMHeadModel\",\n",
            "            ]:\n",
            "                # For XLNet and TransformerXL we add an article to the prompt to give more state to the model.\n",
            "                prefix = self.XL_PREFIX\n",
            "            if prefix is not None:\n",
            "                # Recalculate some generate_kwargs linked to prefix.\n",
            "                preprocess_params, forward_params, _ = self._sanitize_parameters(prefix=prefix, **self._forward_params)\n",
            "                self._preprocess_params = {**self._preprocess_params, **preprocess_params}\n",
            "                self._forward_params = {**self._forward_params, **forward_params}\n",
            "\n",
            "    def _sanitize_parameters(\n",
            "        self,\n",
            "        return_full_text=None,\n",
            "        return_tensors=None,\n",
            "        return_text=None,\n",
            "        return_type=None,\n",
            "        clean_up_tokenization_spaces=None,\n",
            "        prefix=None,\n",
            "        handle_long_generation=None,\n",
            "        stop_sequence=None,\n",
            "        truncation=None,\n",
            "        max_length=None,\n",
            "        continue_final_message=None,\n",
            "        skip_special_tokens=None,\n",
            "        tokenizer_encode_kwargs=None,\n",
            "        **generate_kwargs,\n",
            "    ):\n",
            "        # preprocess kwargs\n",
            "        preprocess_params = {}\n",
            "        add_special_tokens = False\n",
            "        if \"add_special_tokens\" in generate_kwargs:\n",
            "            add_special_tokens = preprocess_params[\"add_special_tokens\"] = generate_kwargs.pop(\"add_special_tokens\")\n",
            "\n",
            "        if \"padding\" in generate_kwargs:\n",
            "            preprocess_params[\"padding\"] = generate_kwargs.pop(\"padding\")\n",
            "\n",
            "        if truncation is not None:\n",
            "            preprocess_params[\"truncation\"] = truncation\n",
            "\n",
            "        if max_length is not None:\n",
            "            preprocess_params[\"max_length\"] = max_length\n",
            "            generate_kwargs[\"max_length\"] = max_length\n",
            "\n",
            "        if prefix is not None:\n",
            "            preprocess_params[\"prefix\"] = prefix\n",
            "        if prefix:\n",
            "            prefix_inputs = self.tokenizer(\n",
            "                prefix, padding=False, add_special_tokens=add_special_tokens, return_tensors=self.framework\n",
            "            )\n",
            "            generate_kwargs[\"prefix_length\"] = prefix_inputs[\"input_ids\"].shape[-1]\n",
            "\n",
            "        if handle_long_generation is not None:\n",
            "            if handle_long_generation != \"hole\":\n",
            "                raise ValueError(\n",
            "                    f\"{handle_long_generation} is not a valid value for `handle_long_generation` parameter expected\"\n",
            "                    \" [None, 'hole']\"\n",
            "                )\n",
            "            preprocess_params[\"handle_long_generation\"] = handle_long_generation\n",
            "\n",
            "        if continue_final_message is not None:\n",
            "            preprocess_params[\"continue_final_message\"] = continue_final_message\n",
            "\n",
            "        if tokenizer_encode_kwargs is not None:\n",
            "            preprocess_params[\"tokenizer_encode_kwargs\"] = tokenizer_encode_kwargs\n",
            "\n",
            "        preprocess_params.update(generate_kwargs)\n",
            "\n",
            "        # forward kwargs\n",
            "        if stop_sequence is not None:\n",
            "            stop_sequence_ids = self.tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
            "            generate_kwargs[\"eos_token_id\"] = stop_sequence_ids\n",
            "        forward_params = generate_kwargs\n",
            "        if self.assistant_model is not None:\n",
            "            forward_params[\"assistant_model\"] = self.assistant_model\n",
            "        if self.assistant_tokenizer is not None:\n",
            "            forward_params[\"tokenizer\"] = self.tokenizer\n",
            "            forward_params[\"assistant_tokenizer\"] = self.assistant_tokenizer\n",
            "\n",
            "        # postprocess kwargs\n",
            "        postprocess_params = {}\n",
            "        if return_full_text is not None and return_type is None:\n",
            "            if return_text is not None:\n",
            "                raise ValueError(\"`return_text` is mutually exclusive with `return_full_text`\")\n",
            "            if return_tensors is not None:\n",
            "                raise ValueError(\"`return_full_text` is mutually exclusive with `return_tensors`\")\n",
            "            return_type = ReturnType.FULL_TEXT if return_full_text else ReturnType.NEW_TEXT\n",
            "        if return_tensors is not None and return_type is None:\n",
            "            if return_text is not None:\n",
            "                raise ValueError(\"`return_text` is mutually exclusive with `return_tensors`\")\n",
            "            return_type = ReturnType.TENSORS\n",
            "        if return_type is not None:\n",
            "            postprocess_params[\"return_type\"] = return_type\n",
            "        if clean_up_tokenization_spaces is not None:\n",
            "            postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n",
            "        if continue_final_message is not None:\n",
            "            postprocess_params[\"continue_final_message\"] = continue_final_message\n",
            "        if skip_special_tokens is not None:\n",
            "            postprocess_params[\"skip_special_tokens\"] = skip_special_tokens\n",
            "\n",
            "        return preprocess_params, forward_params, postprocess_params\n",
            "\n",
            "    # overriding _parse_and_tokenize to allow for unusual language-modeling tokenizer arguments\n",
            "    def _parse_and_tokenize(self, *args, **kwargs):\n",
            "        \"\"\"\n",
            "        Parse arguments and tokenize\n",
            "        \"\"\"\n",
            "        # Parse arguments\n",
            "        if self.model.__class__.__name__ == \"TransfoXLLMHeadModel\":\n",
            "            kwargs.update({\"add_space_before_punct_symbol\": True})\n",
            "\n",
            "        return super()._parse_and_tokenize(*args, **kwargs)\n",
            "\n",
            "    @overload\n",
            "    def __call__(self, text_inputs: str, **kwargs: Any) -> list[dict[str, str]]: ...\n",
            "\n",
            "    @overload\n",
            "    def __call__(self, text_inputs: list[str], **kwargs: Any) -> list[list[dict[str, str]]]: ...\n",
            "\n",
            "    @overload\n",
            "    def __call__(self, text_inputs: ChatType, **kwargs: Any) -> list[dict[str, ChatType]]: ...\n",
            "\n",
            "    @overload\n",
            "    def __call__(self, text_inputs: list[ChatType], **kwargs: Any) -> list[list[dict[str, ChatType]]]: ...\n",
            "\n",
            "    def __call__(self, text_inputs, **kwargs):\n",
            "        \"\"\"\n",
            "        Complete the prompt(s) given as inputs.\n",
            "\n",
            "        Args:\n",
            "            text_inputs (`str`, `list[str]`, list[dict[str, str]], or `list[list[dict[str, str]]]`):\n",
            "                One or several prompts (or one list of prompts) to complete. If strings or a list of string are\n",
            "                passed, this pipeline will continue each prompt. Alternatively, a \"chat\", in the form of a list\n",
            "                of dicts with \"role\" and \"content\" keys, can be passed, or a list of such chats. When chats are passed,\n",
            "                the model's chat template will be used to format them before passing them to the model.\n",
            "            return_tensors (`bool`, *optional*, defaults to `False`):\n",
            "                Returns the tensors of predictions (as token indices) in the outputs. If set to\n",
            "                `True`, the decoded text is not returned.\n",
            "            return_text (`bool`, *optional*):\n",
            "                Returns the decoded texts in the outputs.\n",
            "            return_full_text (`bool`, *optional*, defaults to `True`):\n",
            "                If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n",
            "                specified at the same time as `return_text`.\n",
            "            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n",
            "                Whether or not to clean up the potential extra spaces in the text output.\n",
            "            continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n",
            "                last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n",
            "                By default this is `True` when the final message in the input chat has the `assistant` role and\n",
            "                `False` otherwise, but you can manually override that behaviour by setting this flag.\n",
            "            prefix (`str`, *optional*):\n",
            "                Prefix added to prompt.\n",
            "            handle_long_generation (`str`, *optional*):\n",
            "                By default, this pipelines does not handle long generation (ones that exceed in one form or the other\n",
            "                the model maximum length). There is no perfect way to address this (more info\n",
            "                :https://github.com/huggingface/transformers/issues/14033#issuecomment-948385227). This provides common\n",
            "                strategies to work around that problem depending on your use case.\n",
            "\n",
            "                - `None` : default strategy where nothing in particular happens\n",
            "                - `\"hole\"`: Truncates left of input, and leaves a gap wide enough to let generation happen (might\n",
            "                  truncate a lot of the prompt and not suitable when generation exceed the model capacity)\n",
            "            tokenizer_encode_kwargs (`dict`, *optional*):\n",
            "                Additional keyword arguments to pass along to the encoding step of the tokenizer. If the text input is\n",
            "                a chat, it is passed to `apply_chat_template`. Otherwise, it is passed to `__call__`.\n",
            "            generate_kwargs (`dict`, *optional*):\n",
            "                Additional keyword arguments to pass along to the generate method of the model (see the generate method\n",
            "                corresponding to your framework [here](./text_generation)).\n",
            "\n",
            "        Return:\n",
            "            A list or a list of lists of `dict`: Returns one of the following dictionaries (cannot return a combination\n",
            "            of both `generated_text` and `generated_token_ids`):\n",
            "\n",
            "            - **generated_text** (`str`, present when `return_text=True`) -- The generated text.\n",
            "            - **generated_token_ids** (`torch.Tensor` or `tf.Tensor`, present when `return_tensors=True`) -- The token\n",
            "              ids of the generated text.\n",
            "        \"\"\"\n",
            "        if isinstance(\n",
            "            text_inputs,\n",
            "            (list, tuple, types.GeneratorType, KeyDataset)\n",
            "            if is_torch_available()\n",
            "            else (list, tuple, types.GeneratorType),\n",
            "        ):\n",
            "            if isinstance(text_inputs, types.GeneratorType):\n",
            "                text_inputs, _ = itertools.tee(text_inputs)\n",
            "                text_inputs, first_item = (x for x in text_inputs), next(_)\n",
            "            else:\n",
            "                first_item = text_inputs[0]\n",
            "            if isinstance(first_item, (list, tuple, dict)):\n",
            "                # We have one or more prompts in list-of-dicts format, so this is chat mode\n",
            "                if isinstance(first_item, dict):\n",
            "                    return super().__call__(Chat(text_inputs), **kwargs)\n",
            "                else:\n",
            "                    chats = (Chat(chat) for chat in text_inputs)  # 🐈 🐈 🐈\n",
            "                    if isinstance(text_inputs, types.GeneratorType):\n",
            "                        return super().__call__(chats, **kwargs)\n",
            "                    else:\n",
            "                        return super().__call__(list(chats), **kwargs)\n",
            "        return super().__call__(text_inputs, **kwargs)\n",
            "\n",
            "    def preprocess(\n",
            "        self,\n",
            "        prompt_text,\n",
            "        prefix=\"\",\n",
            "        handle_long_generation=None,\n",
            "        add_special_tokens=None,\n",
            "        truncation=None,\n",
            "        padding=None,\n",
            "        max_length=None,\n",
            "        continue_final_message=None,\n",
            "        tokenizer_encode_kwargs=None,\n",
            "        **generate_kwargs,\n",
            "    ):\n",
            "        # Only set non-None tokenizer kwargs, so as to rely on the tokenizer's defaults\n",
            "        tokenizer_kwargs = {\n",
            "            \"add_special_tokens\": add_special_tokens,\n",
            "            \"truncation\": truncation,\n",
            "            \"padding\": padding,\n",
            "            \"max_length\": max_length,  # NOTE: `max_length` is also a `generate` arg. Use `tokenizer_encode_kwargs` to avoid a name clash\n",
            "        }\n",
            "        tokenizer_kwargs = {key: value for key, value in tokenizer_kwargs.items() if value is not None}\n",
            "        tokenizer_kwargs.update(tokenizer_encode_kwargs or {})\n",
            "\n",
            "        if isinstance(prompt_text, Chat):\n",
            "            tokenizer_kwargs.pop(\"add_special_tokens\", None)  # ignore add_special_tokens on chats\n",
            "            # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n",
            "            # because very few models support multiple separate, consecutive assistant messages\n",
            "            if continue_final_message is None:\n",
            "                continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n",
            "            inputs = self.tokenizer.apply_chat_template(\n",
            "                prompt_text.messages,\n",
            "                add_generation_prompt=not continue_final_message,\n",
            "                continue_final_message=continue_final_message,\n",
            "                return_dict=True,\n",
            "                return_tensors=self.framework,\n",
            "                **tokenizer_kwargs,\n",
            "            )\n",
            "        else:\n",
            "            inputs = self.tokenizer(prefix + prompt_text, return_tensors=self.framework, **tokenizer_kwargs)\n",
            "\n",
            "        inputs[\"prompt_text\"] = prompt_text\n",
            "\n",
            "        if handle_long_generation == \"hole\":\n",
            "            cur_len = inputs[\"input_ids\"].shape[-1]\n",
            "            if \"max_new_tokens\" in generate_kwargs:\n",
            "                new_tokens = generate_kwargs[\"max_new_tokens\"]\n",
            "            else:\n",
            "                new_tokens = generate_kwargs.get(\"max_length\", self.generation_config.max_length) - cur_len\n",
            "                if new_tokens < 0:\n",
            "                    raise ValueError(\"We cannot infer how many new tokens are expected\")\n",
            "            if cur_len + new_tokens > self.tokenizer.model_max_length:\n",
            "                keep_length = self.tokenizer.model_max_length - new_tokens\n",
            "                if keep_length <= 0:\n",
            "                    raise ValueError(\n",
            "                        \"We cannot use `hole` to handle this generation the number of desired tokens exceeds the\"\n",
            "                        \" models max length\"\n",
            "                    )\n",
            "\n",
            "                inputs[\"input_ids\"] = inputs[\"input_ids\"][:, -keep_length:]\n",
            "                if \"attention_mask\" in inputs:\n",
            "                    inputs[\"attention_mask\"] = inputs[\"attention_mask\"][:, -keep_length:]\n",
            "\n",
            "        return inputs\n",
            "\n",
            "    def _forward(self, model_inputs, **generate_kwargs):\n",
            "        input_ids = model_inputs[\"input_ids\"]\n",
            "        attention_mask = model_inputs.get(\"attention_mask\", None)\n",
            "        # Allow empty prompts\n",
            "        if input_ids.shape[1] == 0:\n",
            "            input_ids = None\n",
            "            attention_mask = None\n",
            "            in_b = 1\n",
            "        else:\n",
            "            in_b = input_ids.shape[0]\n",
            "        prompt_text = model_inputs.pop(\"prompt_text\")\n",
            "\n",
            "        # If there is a prefix, we may need to adjust the generation length. Do so without permanently modifying\n",
            "        # generate_kwargs, as some of the parameterization may come from the initialization of the pipeline.\n",
            "        prefix_length = generate_kwargs.pop(\"prefix_length\", 0)\n",
            "        if prefix_length > 0:\n",
            "            has_max_new_tokens = \"max_new_tokens\" in generate_kwargs or (\n",
            "                \"generation_config\" in generate_kwargs\n",
            "                and generate_kwargs[\"generation_config\"].max_new_tokens is not None\n",
            "            )\n",
            "            if not has_max_new_tokens:\n",
            "                generate_kwargs[\"max_length\"] = generate_kwargs.get(\"max_length\") or self.generation_config.max_length\n",
            "                generate_kwargs[\"max_length\"] += prefix_length\n",
            "            has_min_new_tokens = \"min_new_tokens\" in generate_kwargs or (\n",
            "                \"generation_config\" in generate_kwargs\n",
            "                and generate_kwargs[\"generation_config\"].min_new_tokens is not None\n",
            "            )\n",
            "            if not has_min_new_tokens and \"min_length\" in generate_kwargs:\n",
            "                generate_kwargs[\"min_length\"] += prefix_length\n",
            "\n",
            "        # User-defined `generation_config` passed to the pipeline call take precedence\n",
            "        if \"generation_config\" not in generate_kwargs:\n",
            "            generate_kwargs[\"generation_config\"] = self.generation_config\n",
            "\n",
            "        output = self.model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
            "\n",
            "        if isinstance(output, ModelOutput):\n",
            "            generated_sequence = output.sequences\n",
            "            other_outputs = {k: v for k, v in output.items() if k not in {\"sequences\", \"past_key_values\"}}\n",
            "            out_b = generated_sequence.shape[0]\n",
            "\n",
            "            if self.framework == \"pt\":\n",
            "                for key, value in other_outputs.items():\n",
            "                    if isinstance(value, torch.Tensor) and value.shape[0] == out_b:\n",
            "                        other_outputs[key] = value.reshape(in_b, out_b // in_b, *value.shape[1:])\n",
            "                    if isinstance(value, tuple) and len(value[0]) == out_b:\n",
            "                        value = torch.stack(value).swapaxes(0, 1)\n",
            "                        other_outputs[key] = value\n",
            "            elif self.framework == \"tf\":\n",
            "                for key, value in other_outputs.items():\n",
            "                    if isinstance(value, tf.Tensor) and value.shape[0] == out_b:\n",
            "                        other_outputs[key] = tf.reshape(value, (in_b, out_b // in_b, *value.shape[1:]))\n",
            "                    if isinstance(value, tuple) and len(value[0]) == out_b:\n",
            "                        value = tf.stack(value).swapaxes(0, 1)\n",
            "                        other_outputs[key] = value\n",
            "        else:\n",
            "            generated_sequence = output\n",
            "            other_outputs = {}\n",
            "\n",
            "        out_b = generated_sequence.shape[0]\n",
            "        if self.framework == \"pt\":\n",
            "            generated_sequence = generated_sequence.reshape(in_b, out_b // in_b, *generated_sequence.shape[1:])\n",
            "        elif self.framework == \"tf\":\n",
            "            generated_sequence = tf.reshape(generated_sequence, (in_b, out_b // in_b, *generated_sequence.shape[1:]))\n",
            "\n",
            "        model_outputs = {\n",
            "            \"generated_sequence\": generated_sequence,\n",
            "            \"input_ids\": input_ids,\n",
            "            \"prompt_text\": prompt_text,\n",
            "        }\n",
            "        if other_outputs:\n",
            "            model_outputs.update({\"additional_outputs\": other_outputs})\n",
            "        return model_outputs\n",
            "\n",
            "    def postprocess(\n",
            "        self,\n",
            "        model_outputs,\n",
            "        return_type=ReturnType.FULL_TEXT,\n",
            "        clean_up_tokenization_spaces=True,\n",
            "        continue_final_message=None,\n",
            "        skip_special_tokens=None,\n",
            "    ):\n",
            "        generated_sequence = model_outputs[\"generated_sequence\"][0]\n",
            "        input_ids = model_outputs[\"input_ids\"]\n",
            "        prompt_text = model_outputs[\"prompt_text\"]\n",
            "        generated_sequence = generated_sequence.numpy().tolist()\n",
            "        records = []\n",
            "        other_outputs = model_outputs.get(\"additional_outputs\", {})\n",
            "        split_keys = {}\n",
            "        if other_outputs:\n",
            "            if self.framework == \"pt\":\n",
            "                for k, v in other_outputs.items():\n",
            "                    if isinstance(v, torch.Tensor) and v.shape[0] == len(generated_sequence):\n",
            "                        split_keys[k] = v.numpy().tolist()\n",
            "            elif self.framework == \"tf\":\n",
            "                for k, v in other_outputs.items():\n",
            "                    if isinstance(v, tf.Tensor) and v.shape[0] == len(generated_sequence):\n",
            "                        split_keys[k] = v.numpy().tolist()\n",
            "\n",
            "        skip_special_tokens = skip_special_tokens if skip_special_tokens is not None else True\n",
            "        for idx, sequence in enumerate(generated_sequence):\n",
            "            if return_type == ReturnType.TENSORS:\n",
            "                record = {\"generated_token_ids\": sequence}\n",
            "            elif return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n",
            "                # Decode text\n",
            "                text = self.tokenizer.decode(\n",
            "                    sequence,\n",
            "                    skip_special_tokens=skip_special_tokens,\n",
            "                    clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
            "                )\n",
            "\n",
            "                # Remove PADDING prompt of the sequence if XLNet or Transfo-XL model is used\n",
            "                if input_ids is None:\n",
            "                    prompt_length = 0\n",
            "                else:\n",
            "                    prompt_length = len(\n",
            "                        self.tokenizer.decode(\n",
            "                            input_ids[0],\n",
            "                            skip_special_tokens=skip_special_tokens,\n",
            "                            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
            "                        )\n",
            "                    )\n",
            "\n",
            "                all_text = text[prompt_length:]\n",
            "                if return_type == ReturnType.FULL_TEXT:\n",
            "                    if isinstance(prompt_text, str):\n",
            "                        all_text = prompt_text + all_text\n",
            "                    elif isinstance(prompt_text, Chat):\n",
            "                        if continue_final_message is None:\n",
            "                            # If the user passes a chat ending in an assistant message, we treat it as a prefill by\n",
            "                            # default because very few models support multiple separate, consecutive assistant messages\n",
            "                            continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n",
            "                        if continue_final_message:\n",
            "                            # With assistant prefill, concat onto the end of the last message\n",
            "                            all_text = list(prompt_text.messages)[:-1] + [\n",
            "                                {\n",
            "                                    \"role\": prompt_text.messages[-1][\"role\"],\n",
            "                                    \"content\": prompt_text.messages[-1][\"content\"] + all_text,\n",
            "                                }\n",
            "                            ]\n",
            "                        else:\n",
            "                            # When we're not starting from a prefill, the output is a new assistant message\n",
            "                            all_text = list(prompt_text.messages) + [{\"role\": \"assistant\", \"content\": all_text}]\n",
            "                record = {\"generated_text\": all_text}\n",
            "                for key, values in split_keys.items():\n",
            "                    record[key] = values[idx]\n",
            "            records.append(record)\n",
            "\n",
            "        return records\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen3-0.6B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about Rio Grande Valley\"},\n",
        "]\n",
        "pipe(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTmCvNVARxG8",
        "outputId": "f3f1911b-01a4-4972-c148-3654664ca009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': [{'role': 'user',\n",
              "    'content': 'Tell me about Rio Grande Valley'},\n",
              "   {'role': 'assistant',\n",
              "    'content': \"<think>\\nOkay, the user asked about the Rio Grande Valley. Let me start by recalling what I know. The Rio Grande Valley is a region in the U.S., right? It's in Texas, I think. It's known for its agricultural products like corn and beans, and it's a major center for agriculture. There's also the Great Plains region. I should mention both to cover the main points.\\n\\nI need to highlight the agricultural significance. Maybe talk about corn and beans being a staple. Also, the role of the Texas Department of Agriculture. Maybe include something about the cotton industry. The environment is another aspect—because it's part of the Great Plains, it's a region with diverse ecosystems. \\n\\nI should check if there's any other important info. Like, does it have any famous landmarks or cultural aspects? The user might be interested in the historical background. Maybe mention the Texas Revolution or something. Also, the climate, since agriculture is a big part of it. \\n\\nWait, are there any key differences between the Rio Grande Valley and the Great Plains? Probably not, but it's good to note that they are part of the same region. I should keep the answer concise but informative. Make sure to use clear and friendly language, avoiding technical j\"}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or we can do more granular control to manipulate more\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Tell me about Rio Grande Valley\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tmessages,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWGyYWG4Tafn",
        "outputId": "3ce1e748-06d4-4002-90be-4fb87e1e2989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<think>\n",
            "Okay, the user asked about the Rio Grande Valley. First, I need to recall what I know about this region. The Rio Grande Valley is in Texas, right? It's a large part of the state, and I remember it's known for its agriculture, especially cotton and corn. But wait, there's also a region in New Mexico called the Rio Grande Valley. I should confirm that. The user might be mixing up the two regions. I should mention both but clarify the location. Also, the user might be interested in its history, economy, or cultural aspects. Let me structure the answer to cover agriculture, history, and maybe mention the significance of the Rio Grande. Need to make sure the information is accurate and the regions are properly identified. Also, check if there are any other details I should include, like key landmarks or notable events. But since the user just asked for the Rio Grande Valley, focus on that region. Avoid any confusion with New Mexico. Alright, time to put it all together in a clear and concise manner.\n",
            "</think>\n",
            "\n",
            "The **Rio Grande Valley** is a significant agricultural region located in **Texas**, USA. It spans approximately 12,000 square miles and is home to a large portion of Texas's cotton and corn production. The area is known for its fertile soil and access to the **Rio Grande River**, which has historically supported agricultural development and economic growth. \n",
            "\n",
            "### Key Features:\n",
            "1. **Economy**: The region is a major center for cotton and corn farming, with a strong reliance on these crops for both domestic consumption and exports. It also supports other industries like livestock and agro-processing.\n",
            "2. **Historical Significance**: The Rio Grande Valley has played a vital role in Texas's agricultural history, with early settlers and agricultural pioneers contributing to its development.\n",
            "3. **Cultural and Natural Beauty**: The area is rich in natural resources, including water and minerals, and includes historic landmarks such as the **Rio Grande River Bridge** and the **Cotton Palace**. \n",
            "\n",
            "If you're referring to a different region, such as the **Rio Grande Valley in New Mexico**, there might be a distinction. The New Mexico region is distinct and has its own unique history and culture. Let me know if you'd like more details about either!<|im_end|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok this may take a while so let's see how like you can run the server\n",
        "# right why hassle free :)\n",
        "\n",
        "# we will need a codespace\n",
        "# let's go to github\n",
        "# cheat I use, in order not to install libraries independently\n",
        "# pip install vllm\n",
        "# https://docs.vllm.ai/en/stable/\n",
        "# if you have a GPU out of box\n",
        "\n",
        "# create a server.py file\n",
        "\n",
        "# from fastapi import FastAPI\n",
        "# from pydantic import BaseModel\n",
        "# from transformers import pipeline\n",
        "# import uvicorn\n",
        "\n",
        "# app = FastAPI()\n",
        "# pipe = pipeline(\"text-generation\", model=\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
        "\n",
        "# class GenRequest(BaseModel):\n",
        "#     text: str\n",
        "#     max_new_tokens: int = 150\n",
        "#     do_sample: bool = False  # set True if you want to use temperature/top_p, etc.\n",
        "\n",
        "# @app.post(\"/generate\")\n",
        "# def generate(req: GenRequest):\n",
        "#     out = pipe(\n",
        "#         req.text,\n",
        "#         max_new_tokens=req.max_new_tokens,\n",
        "#         do_sample=req.do_sample,\n",
        "#         truncation=True,\n",
        "#         return_full_text=False,\n",
        "#     )\n",
        "#     return {\"generated_text\": out[0][\"generated_text\"]}\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "\n",
        "# uvicorn server:app --host 0.0.0.0 --port 8000 --log-level debug\n",
        "\n",
        "\n",
        "# curl -sS -i http://127.0.0.1:8000/generate \\\n",
        "#   -H \"Content-Type: application/json\" \\\n",
        "#   --data '{\"text\":\"Hello\",\"max_new_tokens\":20}'\n",
        "\n",
        "# curl -sS -i -L \"https://<NAME>-8000.app.github.dev/generate\" \\\n",
        "#   -H \"Content-Type: application/json\" \\\n",
        "#   --data '{\"text\":\"What is the capital of France?\",\"max_new_tokens\":32}'\n",
        "\n",
        "\n",
        "# This works on my machine :))\n",
        "# curl -sS -i -L \"https://verbose-space-lamp-697x5775rr5h7xg-8000.app.github.dev/generate\" \\\n",
        "#   -H \"Content-Type: application/json\" \\\n",
        "#   --data '{\"text\":\"What is the capital of France?\",\"max_new_tokens\":32}'\n"
      ],
      "metadata": {
        "id": "IsKXLTWgVMgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jlMdz1y5WXw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}